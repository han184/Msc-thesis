{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "!pip install xgboost\n",
    "import xgboost\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset for XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    \"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/han/dataframe_engine1-1\",\n",
    "    \"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/han/dataframe_engine1-2\",\n",
    "    \"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/han/dataframe_engine1-3\",\n",
    "]\n",
    "\n",
    "dfs = [pd.read_parquet(path) for path in paths]\n",
    "\n",
    "# Concatenate into one DataFrame\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sort by time\n",
    "combined_df = combined_df.sort_values(by=\"time\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the columns that actually have missing values\n",
    "print(combined_df.isna().sum()[combined_df.isna().sum() == len(combined_df)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying XGBoost for feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/ec2-user/SageMaker/sensor-imputation-thesis/src/sensor_imputation_thesis/han/dataframe_oldid\"\n",
    "df = pd.read_parquet(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the columns that actually have missing values\n",
    "print(df.isna().sum()[df.isna().sum() == len(df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame as the engine is running\n",
    "# since the XGBoost handles missing values so i do not drop nans\n",
    "filtered_df = combined_df[(combined_df['fr_eng'] > (10/60))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a variable as y and other variables as X\n",
    "target_column_list = ['te_air_scav_rec', 'pr_baro', 'pd_air_ic__0', 'pr_exh_rec', 'pr_air_scav_ecs', 're_eng_load', 'te_seawater', 'bo_aux_blower_running', 'in_engine_running_mode']\n",
    "\n",
    "# Define cumulative importance threshold\n",
    "threshold = 0.95\n",
    "\n",
    "for column in target_column_list:\n",
    "    print(f\"\\nTraining for target column: {column}\")\n",
    "\n",
    "    y = filtered_df[column]\n",
    "\n",
    "    # Check if the target column is entirely missing or empty\n",
    "    if y.isna().all() or len(y.dropna()) == 0:\n",
    "        print(f\"Skipping {column}: no available data.\")\n",
    "        continue\n",
    "\n",
    "    # Drop target column and 'time' from features\n",
    "    X = filtered_df.drop(columns=[column, 'time'])\n",
    "\n",
    "    # Split train and test size with chronological order 8:2\n",
    "    train_size = int(len(filtered_df) * 0.8)\n",
    "    X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    mask = y_train.notna() & np.isfinite(y_train)\n",
    "    X_train_clean = X_train[mask]\n",
    "    y_train_clean = y_train[mask]\n",
    "    print(f\"Dropped {len(y_train) - len(y_train_clean)} rows due to NaN or Inf in target.\")\n",
    "\n",
    "    # Define and train model\n",
    "    model = XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=100,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "    # Get importance and normalize (already normalized to sum = 1)\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    # Create a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Add cumulative importance ratio\n",
    "    importance_df['Cumulative'] = importance_df['Importance'].cumsum()\n",
    "\n",
    "    # Filter to top X% of importance\n",
    "    top_features_df = importance_df[importance_df['Cumulative'] <= threshold]\n",
    "\n",
    "    print(f\"\\nTop features covering {threshold*100:.0f}% of total importance:\")\n",
    "    print(top_features_df)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.barh(top_features_df['Feature'], top_features_df['Importance'])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Top {threshold*100:.0f}% Feature Importance for {column}\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
